{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "In this homework, you will apply the TFIDF technique to text classification as well as use word2vec model to generate the dense word embedding for other NLP tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "In this lab, we will experiment different feature extraction on the 20 newgroups dataset, including the count vector and TF-IDF vector. Also, we will apply the Naive Bayes classifier  to this dataset and report the prediciton accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqbOm4jBC92H"
   },
   "source": [
    "### Load the explore the 20newsgroup data\n",
    "\n",
    "20 news group data is part of the sklearn library. We can directly load the data using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cProfile\n",
    "import argparse\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data:11314\n",
      "Number of categories:20\n",
      "From: cubbie@garnet.berkeley.edu (                               )\n",
      "Subject: Re: Cubs behind Marlins? How?\n",
      "Article-I.D.: agate.1pt592$f9a\n",
      "Organization: University of California, Berkeley\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: garnet.berkeley.edu\n",
      "\n",
      "\n",
      "gajarsky@pilot.njin.net writes:\n",
      "\n",
      "morgan and guzman will have era's 1 run higher than last year, and\n",
      " the cubs will be idiots and not pitch harkey as much as hibbard.\n",
      " castillo won't be good (i think he's a stud pitcher)\n",
      "\n",
      "       This season so far, Morgan and Guzman helped to lead the Cubs\n",
      "       at top in ERA, even better than THE rotation at Atlanta.\n",
      "       Cubs ERA at 0.056 while Braves at 0.059. We know it is early\n",
      "       in the season, we Cubs fans have learned how to enjoy the\n",
      "       short triumph while it is still there.\n",
      "\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the traning data and test data\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=False)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=False)\n",
    "\n",
    "# print total number of categories\n",
    "print(\"Number of training data:\" + str(len(twenty_train.data)))\n",
    "print(\"Number of categories:\" + str(len(twenty_train.target_names)))\n",
    "\n",
    "# print the first text and its category\n",
    "print(twenty_train.data[0])\n",
    "print(twenty_train.target[0])\n",
    "\n",
    "# You can check the target variable by printing all the categories\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3db70c26-d684-478a-bcd4-980ed6c6d65b",
    "_uuid": "794fb768f4a8e42c4be4f1dbb27144aae4d00c79",
    "colab_type": "text",
    "id": "FfZcjrp7DWwJ"
   },
   "source": [
    "### Build a Naive Bayes Model \n",
    "\n",
    "Your task is to build practice an ML model to classify the newsgroup data into different categories. You will try both raw count and TF-IDF for feature extraction and then followed by a Naive Bayes classifier. Note that you can connect the feature generation and model training steps into one by using the [pipeline API](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) in sklearn.\n",
    "\n",
    "Try to use Grid Search to find the best hyper parameter from the following settings (feel free to explore other options as well):\n",
    "\n",
    "* Differnet ngram range\n",
    "* Weather or not to remove the stop words\n",
    "* Weather or not to apply IDF\n",
    "\n",
    "After building the best model from the training set, we apply that model to make predictions on the test data and report its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        if step == 'remove_non_ascii':\n",
    "            text = ''.join([x for x in text if ord(x) < 128])\n",
    "        elif step == 'lowercase':\n",
    "            text = text.lower()\n",
    "        elif step == 'remove_punctuation':\n",
    "            punct_exclude = set(string.punctuation)\n",
    "            text = ''.join(char for char in text if char not in punct_exclude)\n",
    "        elif step == 'remove_numbers':\n",
    "            text = re.sub(\"\\d+\", \"\", text)\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "        elif step == 'remove_stopwords':\n",
    "            stops = stopwords.words('english')\n",
    "            word_list = text.split(' ')\n",
    "            text_words = [word for word in word_list if word not in stops]\n",
    "            text = ' '.join(text_words)\n",
    "        elif step == 'stem_words':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "step_list = ['remove_non_ascii', 'lowercase', 'remove_punctuation', 'remove_numbers',\n",
    "            'strip_whitespace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: cubbie@garnet.berkeley.edu (            ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: gnelson@pion.rutgers.edu (Gregory Nelson...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: crypt-comments@math.ncsu.edu\\nSubject: C...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From:  ()\\nSubject: Re: Quadra SCSI Problems??...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: keith@cco.caltech.edu (Keith Allan Schne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  target\n",
       "0  From: cubbie@garnet.berkeley.edu (            ...       9\n",
       "1  From: gnelson@pion.rutgers.edu (Gregory Nelson...       4\n",
       "2  From: crypt-comments@math.ncsu.edu\\nSubject: C...      11\n",
       "3  From:  ()\\nSubject: Re: Quadra SCSI Problems??...       4\n",
       "4  From: keith@cco.caltech.edu (Keith Allan Schne...       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(11314, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.DataFrame({'data':twenty_train.data, 'target':twenty_train.target})\n",
    "test = pd.DataFrame({'data':twenty_test.data, 'target':twenty_test.target})\n",
    "train.head()\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer()\n",
    "X_train = tf.fit_transform(train['data'])\n",
    "y_train = train['target']\n",
    "X_test = tf.transform(test['data'])\n",
    "y_test = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_STOP = TfidfVectorizer(stop_words='english')\n",
    "X_train_STOP = tf_STOP.fit_transform(train['data'])\n",
    "y_train_STOP = train['target']\n",
    "X_test_STOP = tf_STOP.transform(test['data'])\n",
    "y_test_STOP = test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = train.copy()\n",
    "train_clean['data'] = train_clean['data'].map(lambda s: preprocess(s, step_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
       "                ('gridsearchcv',\n",
       "                 GridSearchCV(estimator=MultinomialNB(),\n",
       "                              param_grid={'alpha': [0.1, 0.2, 0.3, 0.4, 0.5,\n",
       "                                                    0.6, 0.7, 0.8, 0.9, 1]}))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper parameter setting is {'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Grid Search\n",
    "parameters = {\n",
    "    'alpha': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "}\n",
    "nb_grid = GridSearchCV(MultinomialNB(), parameters)\n",
    "\n",
    "# Models \n",
    "# nb = MultinomialNB()\n",
    "# nb_TFIDF = make_pipeline(tf, nb_grid)\n",
    "# nb_TFIDF_STOP = make_pipeline(tf_STOP, nb_grid)\n",
    "nb_TFIDF_CLEAN = make_pipeline(TfidfVectorizer(), nb_grid)\n",
    "\n",
    "# Train\n",
    "# nb_TFIDF.fit(X_train, y_train)\n",
    "# nb_TFIDF_STOP.fit(X_train_STOP, y_train_STOP)\n",
    "nb_TFIDF_CLEAN.fit(train_clean['data'], train_clean['target'])\n",
    "print(f\"The best hyper parameter setting is {nb_grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE from TFIDF\n",
    "# accuracy_score(y_true, y_pred)\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# y_pred_TFIDF = nb_grid.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred_TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMANCE from TFIDF with Stop Words\n",
    "# accuracy_score(y_true, y_pred)\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# y_pred_STOP = nb_TFIDF_STOP.predict(twenty_test.data)\n",
    "# accuracy_score(twenty_test.target, y_pred_STOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF accuracy from clean data: 0.8128\n"
     ]
    }
   ],
   "source": [
    "# PERFORMANCE from TFIDF with Clean Data\n",
    "# accuracy_score(y_true, y_pred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_CLEAN = nb_TFIDF_CLEAN.predict(twenty_test.data)\n",
    "print(f'TF-IDF accuracy from clean data: {\"%0.4F\" % accuracy_score(twenty_test.target, y_pred_CLEAN)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html\n",
    "# categories = ['talk.religion.misc', 'soc.religion.christian',\n",
    "#              'sci.space', 'comp.graphics']\n",
    "# train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "# test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "# model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "# model.fit(train.data, train.target)\n",
    "# y_pred = model.predict(test.data)\n",
    "# accuracy_score(test.target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding with word2vec\n",
    "\n",
    "Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. \n",
    "\n",
    "In this assessment, we will experiment with [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) model from package [gensim](https://radimrehurek.com/gensim/) and generate word embeddings from a review dataset. You can then explore those word embeddings and see if they make sense semantically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import logging\n",
    "import warnings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file ../data/reviews_data.txt.gz...this may take a while\n",
      "read 0 reviews\n",
      "read 10000 reviews\n",
      "read 20000 reviews\n",
      "read 30000 reviews\n",
      "read 40000 reviews\n",
      "read 50000 reviews\n",
      "read 60000 reviews\n",
      "read 70000 reviews\n",
      "read 80000 reviews\n",
      "read 90000 reviews\n",
      "read 100000 reviews\n",
      "read 110000 reviews\n",
      "read 120000 reviews\n",
      "read 130000 reviews\n",
      "read 140000 reviews\n",
      "read 150000 reviews\n",
      "read 160000 reviews\n",
      "read 170000 reviews\n",
      "read 180000 reviews\n",
      "read 190000 reviews\n",
      "read 200000 reviews\n",
      "read 210000 reviews\n",
      "read 220000 reviews\n",
      "read 230000 reviews\n",
      "read 240000 reviews\n",
      "read 250000 reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 23:33:36,235 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    print(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    with gzip.open(input_file, 'rb') as f:\n",
    "        for i, line in enumerate(f):\n",
    " \n",
    "            if (i % 10000 == 0):\n",
    "                print(\"read {0} reviews\".format(i))\n",
    "            # do some pre-processing and return list of words for each review b text\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "            \n",
    "documents = list(read_input('../data/reviews_data.txt.gz'))\n",
    "logging.info(\"Done reading data file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the word2vec model\n",
    "\n",
    "The word2vec algorithms include skip-gram and CBOW models, using either hierarchical softmax or negative sampling introduced in Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. A word2vec tutorial can be found [here](https://rare-technologies.com/word2vec-tutorial/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 23:33:36,238 : INFO : collecting all words and their counts\n",
      "2021-03-01 23:33:36,239 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-03-01 23:33:36,419 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2021-03-01 23:33:36,605 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2021-03-01 23:33:36,823 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2021-03-01 23:33:37,026 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2021-03-01 23:33:37,248 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2021-03-01 23:33:37,500 : INFO : PROGRESS: at sentence #60000, processed 11013726 words, keeping 76786 word types\n",
      "2021-03-01 23:33:37,707 : INFO : PROGRESS: at sentence #70000, processed 12637528 words, keeping 83199 word types\n",
      "2021-03-01 23:33:37,896 : INFO : PROGRESS: at sentence #80000, processed 14099754 words, keeping 88459 word types\n",
      "2021-03-01 23:33:38,073 : INFO : PROGRESS: at sentence #90000, processed 15662152 words, keeping 93357 word types\n",
      "2021-03-01 23:33:38,242 : INFO : PROGRESS: at sentence #100000, processed 17164490 words, keeping 97886 word types\n",
      "2021-03-01 23:33:38,409 : INFO : PROGRESS: at sentence #110000, processed 18652295 words, keeping 102132 word types\n",
      "2021-03-01 23:33:38,578 : INFO : PROGRESS: at sentence #120000, processed 20152532 words, keeping 105923 word types\n",
      "2021-03-01 23:33:38,752 : INFO : PROGRESS: at sentence #130000, processed 21684333 words, keeping 110104 word types\n",
      "2021-03-01 23:33:38,937 : INFO : PROGRESS: at sentence #140000, processed 23330209 words, keeping 114108 word types\n",
      "2021-03-01 23:33:39,108 : INFO : PROGRESS: at sentence #150000, processed 24838757 words, keeping 118174 word types\n",
      "2021-03-01 23:33:39,283 : INFO : PROGRESS: at sentence #160000, processed 26390913 words, keeping 118670 word types\n",
      "2021-03-01 23:33:39,473 : INFO : PROGRESS: at sentence #170000, processed 27913919 words, keeping 123356 word types\n",
      "2021-03-01 23:33:39,675 : INFO : PROGRESS: at sentence #180000, processed 29535615 words, keeping 126748 word types\n",
      "2021-03-01 23:33:39,851 : INFO : PROGRESS: at sentence #190000, processed 31096462 words, keeping 129847 word types\n",
      "2021-03-01 23:33:40,043 : INFO : PROGRESS: at sentence #200000, processed 32805274 words, keeping 133255 word types\n",
      "2021-03-01 23:33:40,225 : INFO : PROGRESS: at sentence #210000, processed 34434201 words, keeping 136364 word types\n",
      "2021-03-01 23:33:40,411 : INFO : PROGRESS: at sentence #220000, processed 36083485 words, keeping 139418 word types\n",
      "2021-03-01 23:33:40,586 : INFO : PROGRESS: at sentence #230000, processed 37571765 words, keeping 142399 word types\n",
      "2021-03-01 23:33:40,772 : INFO : PROGRESS: at sentence #240000, processed 39138193 words, keeping 145232 word types\n",
      "2021-03-01 23:33:40,949 : INFO : PROGRESS: at sentence #250000, processed 40695052 words, keeping 147966 word types\n",
      "2021-03-01 23:33:41,048 : INFO : collected 150059 word types from a corpus of 41519358 raw words and 255404 sentences\n",
      "2021-03-01 23:33:41,048 : INFO : Loading a fresh vocabulary\n",
      "2021-03-01 23:33:41,132 : INFO : effective_min_count=5 retains 38581 unique words (25% of original 150059, drops 111478)\n",
      "2021-03-01 23:33:41,132 : INFO : effective_min_count=5 leaves 41357364 word corpus (99% of original 41519358, drops 161994)\n",
      "2021-03-01 23:33:41,208 : INFO : deleting the raw counts dictionary of 150059 items\n",
      "2021-03-01 23:33:41,211 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2021-03-01 23:33:41,212 : INFO : downsampling leaves estimated 30256217 word corpus (73.2% of prior 41357364)\n",
      "2021-03-01 23:33:41,297 : INFO : estimated required memory for 38581 words and 100 dimensions: 50155300 bytes\n",
      "2021-03-01 23:33:41,297 : INFO : resetting layer weights\n",
      "2021-03-01 23:33:46,559 : INFO : training model with 3 workers on 38581 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-03-01 23:33:47,566 : INFO : EPOCH 1 - PROGRESS: at 4.65% examples, 1425952 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:33:48,572 : INFO : EPOCH 1 - PROGRESS: at 9.35% examples, 1447094 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:33:49,572 : INFO : EPOCH 1 - PROGRESS: at 13.32% examples, 1451711 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:33:50,579 : INFO : EPOCH 1 - PROGRESS: at 17.40% examples, 1445852 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:33:51,580 : INFO : EPOCH 1 - PROGRESS: at 21.54% examples, 1449024 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:33:52,584 : INFO : EPOCH 1 - PROGRESS: at 25.65% examples, 1450328 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:33:53,592 : INFO : EPOCH 1 - PROGRESS: at 31.04% examples, 1452508 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:33:54,598 : INFO : EPOCH 1 - PROGRESS: at 36.15% examples, 1451799 words/s, in_qsize 5, out_qsize 1\n",
      "2021-03-01 23:33:55,601 : INFO : EPOCH 1 - PROGRESS: at 41.47% examples, 1454228 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:33:56,602 : INFO : EPOCH 1 - PROGRESS: at 46.73% examples, 1455139 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:33:57,607 : INFO : EPOCH 1 - PROGRESS: at 51.72% examples, 1452618 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:33:58,610 : INFO : EPOCH 1 - PROGRESS: at 56.35% examples, 1445246 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:33:59,613 : INFO : EPOCH 1 - PROGRESS: at 61.29% examples, 1443414 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:00,618 : INFO : EPOCH 1 - PROGRESS: at 66.51% examples, 1445863 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:01,624 : INFO : EPOCH 1 - PROGRESS: at 71.03% examples, 1441262 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:02,629 : INFO : EPOCH 1 - PROGRESS: at 75.74% examples, 1437078 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:03,629 : INFO : EPOCH 1 - PROGRESS: at 80.45% examples, 1439125 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:04,630 : INFO : EPOCH 1 - PROGRESS: at 84.76% examples, 1432378 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:05,634 : INFO : EPOCH 1 - PROGRESS: at 89.68% examples, 1430197 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:06,639 : INFO : EPOCH 1 - PROGRESS: at 94.62% examples, 1429930 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:07,640 : INFO : EPOCH 1 - PROGRESS: at 99.53% examples, 1429600 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:07,719 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-01 23:34:07,725 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-01 23:34:07,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-01 23:34:07,729 : INFO : EPOCH - 1 : training on 41519358 raw words (30260399 effective words) took 21.2s, 1429544 effective words/s\n",
      "2021-03-01 23:34:08,741 : INFO : EPOCH 2 - PROGRESS: at 4.41% examples, 1339465 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:09,742 : INFO : EPOCH 2 - PROGRESS: at 8.82% examples, 1358125 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:10,751 : INFO : EPOCH 2 - PROGRESS: at 12.65% examples, 1380984 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:11,754 : INFO : EPOCH 2 - PROGRESS: at 16.75% examples, 1382874 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:12,761 : INFO : EPOCH 2 - PROGRESS: at 20.48% examples, 1386517 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:13,766 : INFO : EPOCH 2 - PROGRESS: at 24.55% examples, 1398795 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:14,772 : INFO : EPOCH 2 - PROGRESS: at 30.01% examples, 1411370 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:15,773 : INFO : EPOCH 2 - PROGRESS: at 35.25% examples, 1420316 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:16,775 : INFO : EPOCH 2 - PROGRESS: at 40.31% examples, 1420153 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:17,778 : INFO : EPOCH 2 - PROGRESS: at 45.17% examples, 1409318 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:18,780 : INFO : EPOCH 2 - PROGRESS: at 49.44% examples, 1392054 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:19,783 : INFO : EPOCH 2 - PROGRESS: at 53.59% examples, 1380888 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 23:34:20,794 : INFO : EPOCH 2 - PROGRESS: at 57.65% examples, 1361776 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:21,797 : INFO : EPOCH 2 - PROGRESS: at 62.49% examples, 1364519 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:22,799 : INFO : EPOCH 2 - PROGRESS: at 67.28% examples, 1364085 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:23,807 : INFO : EPOCH 2 - PROGRESS: at 71.83% examples, 1365253 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:24,815 : INFO : EPOCH 2 - PROGRESS: at 76.53% examples, 1366640 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:25,818 : INFO : EPOCH 2 - PROGRESS: at 80.88% examples, 1365570 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:26,823 : INFO : EPOCH 2 - PROGRESS: at 85.47% examples, 1367259 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:27,836 : INFO : EPOCH 2 - PROGRESS: at 90.61% examples, 1369717 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:28,838 : INFO : EPOCH 2 - PROGRESS: at 95.38% examples, 1370506 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:29,781 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-01 23:34:29,783 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-01 23:34:29,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-01 23:34:29,786 : INFO : EPOCH - 2 : training on 41519358 raw words (30260299 effective words) took 22.1s, 1372010 effective words/s\n",
      "2021-03-01 23:34:30,794 : INFO : EPOCH 3 - PROGRESS: at 4.48% examples, 1372998 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:31,803 : INFO : EPOCH 3 - PROGRESS: at 9.08% examples, 1401363 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:32,804 : INFO : EPOCH 3 - PROGRESS: at 13.08% examples, 1425193 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:33,808 : INFO : EPOCH 3 - PROGRESS: at 17.30% examples, 1435932 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:34,812 : INFO : EPOCH 3 - PROGRESS: at 21.02% examples, 1433658 words/s, in_qsize 5, out_qsize 1\n",
      "2021-03-01 23:34:35,813 : INFO : EPOCH 3 - PROGRESS: at 25.25% examples, 1431847 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:36,815 : INFO : EPOCH 3 - PROGRESS: at 30.62% examples, 1437524 words/s, in_qsize 3, out_qsize 2\n",
      "2021-03-01 23:34:37,817 : INFO : EPOCH 3 - PROGRESS: at 35.86% examples, 1443773 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:38,819 : INFO : EPOCH 3 - PROGRESS: at 41.22% examples, 1448074 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:39,822 : INFO : EPOCH 3 - PROGRESS: at 46.57% examples, 1450861 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:40,824 : INFO : EPOCH 3 - PROGRESS: at 51.72% examples, 1453685 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:41,827 : INFO : EPOCH 3 - PROGRESS: at 56.74% examples, 1455643 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:42,831 : INFO : EPOCH 3 - PROGRESS: at 61.86% examples, 1457334 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:43,834 : INFO : EPOCH 3 - PROGRESS: at 67.08% examples, 1458858 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:44,837 : INFO : EPOCH 3 - PROGRESS: at 71.95% examples, 1460450 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:45,838 : INFO : EPOCH 3 - PROGRESS: at 76.87% examples, 1460750 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:46,839 : INFO : EPOCH 3 - PROGRESS: at 81.63% examples, 1461705 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:47,841 : INFO : EPOCH 3 - PROGRESS: at 86.58% examples, 1462850 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:48,848 : INFO : EPOCH 3 - PROGRESS: at 91.84% examples, 1463415 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:49,851 : INFO : EPOCH 3 - PROGRESS: at 96.92% examples, 1464103 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:50,435 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-01 23:34:50,441 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-01 23:34:50,444 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-01 23:34:50,444 : INFO : EPOCH - 3 : training on 41519358 raw words (30255890 effective words) took 20.7s, 1464699 effective words/s\n",
      "2021-03-01 23:34:51,455 : INFO : EPOCH 4 - PROGRESS: at 4.83% examples, 1469618 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:52,463 : INFO : EPOCH 4 - PROGRESS: at 9.51% examples, 1474094 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:53,466 : INFO : EPOCH 4 - PROGRESS: at 13.58% examples, 1475092 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:54,471 : INFO : EPOCH 4 - PROGRESS: at 17.72% examples, 1475036 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:55,471 : INFO : EPOCH 4 - PROGRESS: at 21.92% examples, 1476617 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:56,477 : INFO : EPOCH 4 - PROGRESS: at 26.23% examples, 1474530 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:57,479 : INFO : EPOCH 4 - PROGRESS: at 31.70% examples, 1475104 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:34:58,480 : INFO : EPOCH 4 - PROGRESS: at 36.84% examples, 1476687 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:34:59,485 : INFO : EPOCH 4 - PROGRESS: at 42.14% examples, 1473780 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:00,486 : INFO : EPOCH 4 - PROGRESS: at 47.25% examples, 1470755 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:35:01,487 : INFO : EPOCH 4 - PROGRESS: at 52.23% examples, 1467206 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:02,489 : INFO : EPOCH 4 - PROGRESS: at 57.29% examples, 1467669 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:35:03,496 : INFO : EPOCH 4 - PROGRESS: at 62.41% examples, 1468705 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:04,502 : INFO : EPOCH 4 - PROGRESS: at 67.66% examples, 1469575 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:05,504 : INFO : EPOCH 4 - PROGRESS: at 72.53% examples, 1470113 words/s, in_qsize 6, out_qsize 1\n",
      "2021-03-01 23:35:06,514 : INFO : EPOCH 4 - PROGRESS: at 77.41% examples, 1470254 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:35:07,514 : INFO : EPOCH 4 - PROGRESS: at 82.23% examples, 1470328 words/s, in_qsize 4, out_qsize 1\n",
      "2021-03-01 23:35:08,517 : INFO : EPOCH 4 - PROGRESS: at 87.21% examples, 1470912 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:09,518 : INFO : EPOCH 4 - PROGRESS: at 92.41% examples, 1470411 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:35:10,524 : INFO : EPOCH 4 - PROGRESS: at 97.46% examples, 1470494 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:11,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-01 23:35:11,080 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-01 23:35:11,085 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-01 23:35:11,086 : INFO : EPOCH - 4 : training on 41519358 raw words (30255641 effective words) took 20.6s, 1465821 effective words/s\n",
      "2021-03-01 23:35:12,091 : INFO : EPOCH 5 - PROGRESS: at 3.90% examples, 1196789 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:13,092 : INFO : EPOCH 5 - PROGRESS: at 8.69% examples, 1337119 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:35:14,094 : INFO : EPOCH 5 - PROGRESS: at 12.34% examples, 1348554 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:15,101 : INFO : EPOCH 5 - PROGRESS: at 16.48% examples, 1361131 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:16,101 : INFO : EPOCH 5 - PROGRESS: at 20.36% examples, 1382214 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:17,107 : INFO : EPOCH 5 - PROGRESS: at 24.44% examples, 1393924 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:35:18,117 : INFO : EPOCH 5 - PROGRESS: at 29.85% examples, 1405355 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:19,119 : INFO : EPOCH 5 - PROGRESS: at 35.04% examples, 1414895 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:35:20,120 : INFO : EPOCH 5 - PROGRESS: at 40.29% examples, 1421154 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:21,124 : INFO : EPOCH 5 - PROGRESS: at 45.72% examples, 1425753 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:22,131 : INFO : EPOCH 5 - PROGRESS: at 50.92% examples, 1431632 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:23,137 : INFO : EPOCH 5 - PROGRESS: at 55.94% examples, 1434810 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:35:24,145 : INFO : EPOCH 5 - PROGRESS: at 61.08% examples, 1437700 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-01 23:35:25,146 : INFO : EPOCH 5 - PROGRESS: at 66.24% examples, 1440486 words/s, in_qsize 6, out_qsize 0\n",
      "2021-03-01 23:35:26,146 : INFO : EPOCH 5 - PROGRESS: at 71.11% examples, 1443046 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:27,149 : INFO : EPOCH 5 - PROGRESS: at 76.15% examples, 1445970 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:28,151 : INFO : EPOCH 5 - PROGRESS: at 80.92% examples, 1448160 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:29,151 : INFO : EPOCH 5 - PROGRESS: at 85.77% examples, 1449751 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:30,157 : INFO : EPOCH 5 - PROGRESS: at 91.06% examples, 1450692 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:31,162 : INFO : EPOCH 5 - PROGRESS: at 96.15% examples, 1452317 words/s, in_qsize 5, out_qsize 0\n",
      "2021-03-01 23:35:31,904 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-03-01 23:35:31,908 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-03-01 23:35:31,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-03-01 23:35:31,910 : INFO : EPOCH - 5 : training on 41519358 raw words (30256994 effective words) took 20.8s, 1453052 effective words/s\n",
      "2021-03-01 23:35:31,910 : INFO : training on a 207596790 raw words (151289223 effective words) took 105.4s, 1436051 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# TODO build vocabulary and train model\n",
    "model = gensim.models.Word2Vec(documents, min_count=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar words for a given word\n",
    "Once the model is built, you can find interesting patterns in the model. For example, can you find the 5 most similar words to word `polite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kegedy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "2021-03-01 23:35:31,915 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9425445795059204),\n",
       " ('cordial', 0.912461519241333),\n",
       " ('curteous', 0.9069803953170776),\n",
       " ('curtious', 0.8921737670898438),\n",
       " ('personable', 0.8755834698677063)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: look up top 5 words similar to 'polite' using most_similar function\n",
    "# Feel free to try other words and see if it makes sense.\n",
    "model.most_similar(positive=['polite'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the word embedding by comparing their similarities\n",
    "We can also find similarity betwen two words in the embedding space. Can you find the similarities between word `great` and `good`/`horrible`, and also `dirty` and `clean`/`smelly`. Feel free to play around with the word embedding you just learnt and see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good similarity: 0.8299\n",
      "great and horrible similarity: 0.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kegedy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"\n",
      "/home/kegedy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# TODO: find similarities between two words using similarity function\n",
    "a = 'great'\n",
    "b = 'good'\n",
    "c = 'horrible'\n",
    "print(f'{a} and {b} similarity: {\"%0.4f\" % model.similarity(a, b)}')\n",
    "print(f'{a} and {c} similarity: {\"%0.4f\" % model.similarity(a, c)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirty and clean similarity: 0.3428\n",
      "dirty and smelly similarity: 0.7966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kegedy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"\n",
      "/home/kegedy/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# TODO: find similarities between two words using similarity function\n",
    "a = 'dirty'\n",
    "b = 'clean'\n",
    "c = 'smelly'\n",
    "print(f'{a} and {b} similarity: {\"%0.4f\" % model.similarity(a, b)}')\n",
    "print(f'{a} and {c} similarity: {\"%0.4f\" % model.similarity(a, c)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Reflection</b>\n",
    "<p>\n",
    "This week I learned how powerful natural learning processing is. It is incredibly useful in quantifying vocabularies in order to find insights. I greatly enjoyed diving into this dataset and getting my hands dirty.\n",
    "<p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
